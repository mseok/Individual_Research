{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get maximum, minimum energy positions of molecule\n",
    "def get_positions_from_mol(mol, e_min):\n",
    "    # Generate conformers and calculate their energy\n",
    "    AllChem.EmbedMultipleConfs(mol, 10)\n",
    "    idx_energy_list = AllChem.UFFOptimizeMoleculeConfs(mol, maxIters=1000)\n",
    "\n",
    "    # Get position of conformers with maximum, minimum energy\n",
    "    energy_list = [el[1] for el in idx_energy_list]\n",
    "    if e_min:\n",
    "        energy = min(energy_list)   \n",
    "        print(\"min Energy: \", energy)\n",
    "    else:\n",
    "        energy = max(energy_list)\n",
    "        print(\"max Energy: \", energy)\n",
    "    idx = energy_list.index(energy)\n",
    "    conformer = mol.GetConformer(idx)\n",
    "    pos = conformer.GetPositions()\n",
    "\n",
    "    # Convert position values to torch tensors\n",
    "    pos = torch.FloatTensor(pos)\n",
    "\n",
    "    # Set gradient to max_pos variable (we will optimize max_pos to min_pos)\n",
    "    if e_min:\n",
    "        print(\"Target position:\")\n",
    "    else:\n",
    "        pos = pos.float().requires_grad_(True)\n",
    "        print(\"Initial position with gradient:\")\n",
    "    print(pos)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for doing iteration\n",
    "def optimize(pos, target, zero_grad=True, loss_type=\"l2\", it=1000, lr=0.01):\n",
    "    optimizer = optim.Adam([pos], lr=lr)\n",
    "    for i in range(it):\n",
    "        if zero_grad:\n",
    "            optimizer.zero_grad()\n",
    "        # L2 loss\n",
    "        if loss_type == \"l2\":\n",
    "            loss = F.mse_loss(pos.view(-1), target.view(-1))\n",
    "        if loss_type == \"l1\":\n",
    "            loss = torch.abs(pos-target).view(-1).sum(-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss < 0.01:\n",
    "            print(f\"Loss lower than 0.01 at {i}th iteration.\")\n",
    "            print(loss)\n",
    "            print(pos)\n",
    "            break\n",
    "        if i == it-1:\n",
    "            print(f\"End of the {it} iteration.\")\n",
    "            print(loss)\n",
    "            print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get molecule\n",
    "smiles = \"CCCC\"\n",
    "mol = Chem.MolFromSmiles(smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without optimzer zero gradient\n",
    "If we do not intialize the optimizer's gradient as zero, the gradient will be accumulated and it is hard to properly update the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min Energy:  -0.10454262826526749\n",
      "Target position:\n",
      "tensor([[-1.8928, -0.2013, -0.0800],\n",
      "        [-0.5562,  0.5097, -0.0628],\n",
      "        [ 0.5562, -0.5097,  0.0628],\n",
      "        [ 1.8928,  0.2013,  0.0800]])\n"
     ]
    }
   ],
   "source": [
    "# Position with minimum energy\n",
    "min_pos = get_positions_from_mol(mol, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Energy:  1.0094345718047124\n",
      "Initial position with gradient:\n",
      "tensor([[ 1.5559, -0.5532, -0.1156],\n",
      "        [ 0.6551,  0.5610,  0.3836],\n",
      "        [-0.6601,  0.5608, -0.3753],\n",
      "        [-1.5509, -0.5687,  0.1073]], requires_grad=True)\n",
      "End of the 1000 iteration.\n",
      "tensor(1.5046, grad_fn=<MseLossBackward>)\n",
      "tensor([[ 0.2058, -1.5506,  1.3489],\n",
      "        [-0.3643,  0.3310, -0.6021],\n",
      "        [ 0.3042, -1.7829,  0.4125],\n",
      "        [-0.2131, -1.1532, -1.1121]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "max_pos_no_opt = get_positions_from_mol(mol, False)\n",
    "optimize(max_pos_no_opt, min_pos, zero_grad=False, loss_type=\"l2\", it=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With optimzer zero gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Energy:  1.009434572280784\n",
      "Initial position with gradient:\n",
      "tensor([[-1.5476, -0.5630,  0.1674],\n",
      "        [-0.6718,  0.5615, -0.3527],\n",
      "        [ 0.6707,  0.5603,  0.3566],\n",
      "        [ 1.5487, -0.5588, -0.1713]], requires_grad=True)\n",
      "Loss lower than 0.01 at 90th iteration.\n",
      "tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "tensor([[-1.8935, -0.1991, -0.0785],\n",
      "        [-0.5560,  0.5100, -0.0655],\n",
      "        [ 0.5559, -0.1859,  0.0654],\n",
      "        [ 1.8934,  0.1013,  0.0783]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "max_pos = get_positions_from_mol(mol, False)\n",
    "optimize(max_pos, min_pos, zero_grad=True, loss_type=\"l2\", it=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Loss vs L1 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 loss terms can be expressed as following equations.\n",
    "$$L_1 = \\sum^n_{i=1} |y_i - f\\left( x_i\\right)|$$\n",
    "$$L_2 = \\sum^n_{i=1} \\left( y_i - f\\left(x_i \\right) \\right)^2$$\n",
    "The gradient of tensor is the derivative of loss terms with respect to each element in tensor. The derivative of L1 and L2 loss can be expressed as following equations.\n",
    "$${\\operatorname{d}\\!L_1\\over\\operatorname{d}\\!x} = \\pm 1$$\n",
    "$${\\operatorname{d}\\!L_2\\over\\operatorname{d}\\!x} = -2f'\\left(x\\right)f\\left(x\\right)$$\n",
    "Since the derivative of L1 loss only can have either +1 or -1, finding the local minimum with L1 loss is harder than L2 loss, which has continuous derivative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Energy:  1.0094345710523354\n",
      "Initial position with gradient:\n",
      "tensor([[-1.5034,  0.6378,  0.2702],\n",
      "        [-0.7215, -0.5396, -0.2815],\n",
      "        [ 0.6719, -0.5805,  0.3203],\n",
      "        [ 1.5531,  0.4823, -0.3090]], requires_grad=True)\n",
      "Loss lower than 0.01 at 128th iteration.\n",
      "tensor(0.0075, grad_fn=<SumBackward1>)\n",
      "tensor([[-1.8919, -0.2018, -0.0792],\n",
      "        [-0.5557,  0.5130, -0.0633],\n",
      "        [ 0.5572, -0.5090,  0.0633],\n",
      "        [ 1.8921,  0.2030,  0.0791]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "max_pos_l1 = get_positions_from_mol(mol, False)\n",
    "optimize(max_pos_l1, min_pos, zero_grad=True, loss_type=\"l1\", it=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **forward()** function in the torch computes the value of loss functions while **backward()** function computes the gradients of the learnable parameters with respect to loss.\n",
    "\n",
    "The **Graph** in torch grad package is a copmutational graph which nodes are composed with mathematical operators such as sum or multiply, except for the case that user-defined variable. The **Leaf** in torch means the variables that every initial variables, not the result of mathematical operations. For example, suppose that there's a graph with following equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_1 = w_1 \\times x_1$$\n",
    "$$d_2 = w_2 \\times x_2$$\n",
    "$$d_3 = d_1 + d_2$$\n",
    "$$y = w_3 \\times d_3$$\n",
    "$$L = 10 - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the graph consists of $\\times$, $+$, and $-$. Also, $x_1$, $x_2$, $w_1$, $w_2$, and $w_3$ are leaves of the graph that initialized by users. The following is the definition of leaf in torch documentation.\n",
    "- All Tensors that have *requires_grad* which is **False** will be leaf Tensors by convention.\n",
    "- For Tensors that have *requires_grad* which is **True**, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so **grad_fn** is None.\n",
    "- Only leaf Tensors will have their *grad* populated during a call to *backward()*. To get *grad* populated for non-leaf Tensors, you can use *retain_grad()*.\n",
    "\n",
    "Therefore, the definition in **backward()** in torch documentation: \"Computes the gradient of current tensor w.r.t. graph leaves.\" means that torch automatically computes the derivatives of loss w.r.t. graph leaves via chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10, requires_grad=True)\n",
    "a.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b is created by the addition operation\n",
    "b = torch.rand(10, requires_grad=True) + 2\n",
    "b.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.autograd.grad** act slightly different with **torch.autograd.backward**. The latter computes the sum of gradients of given tensors w.r.t. graph leaves, and the former computes and returns the sum of the gradients of outputs w.r.t. the inputs.\n",
    "\n",
    "Both **grad()** and **backward()** has options of *retain_graph* and *create_graph*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With *create_graph=True*, we are declaring that we want to do further operations on gradients, so that the autograd engine can create a backpropable graph for operations done on gradients. *retain_graph=True* declares that we will want to reuse the overall graph multiple times, so do not delete it after someone called **backward()**. During the **backward()**, torch goes backward multiple times of computation graph (sum, square, MSE, view, sum in below example). If we do not use *retain_graph=True*, then the gradients will be vanished after computing first derivative computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for generating grad objects\n",
    "def generate_grad(retain_graph, create_graph):\n",
    "    input = torch.ones(2, 2, requires_grad=True)\n",
    "    pred = input + 2\n",
    "    pred = pred ** 2\n",
    "    target = torch.ones_like(input) * 9 - 0.2\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    loss = loss_fn(pred, target)\n",
    "    print(f\"Loss: {loss:.3f}\")\n",
    "    \n",
    "    gradient = torch.autograd.grad(outputs=loss, inputs=input, retain_graph=retain_graph, create_graph=create_graph)\n",
    "    print(f\"dloss/dinput:\\n {gradient}\")\n",
    "    return input, gradient[0].view(-1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.040\n",
      "dloss/dinput:\n",
      " (tensor([[0.6000, 0.6000],\n",
      "        [0.6000, 0.6000]]),)\n",
      "gradient without retain graph and create graph: 2.400\n",
      "#### EXCEPTION:  element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "input, gradient = generate_grad(False, False)\n",
    "print(f\"gradient without retain graph and create graph: {gradient:.3f}\")\n",
    "try:\n",
    "    gradient.backward()\n",
    "    print(input) # Graph did not created therefore cannot do backward() operation\n",
    "except RuntimeError as RE:\n",
    "    print(\"#### EXCEPTION: \", RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.040\n",
      "dloss/dinput:\n",
      " (tensor([[0.6000, 0.6000],\n",
      "        [0.6000, 0.6000]]),)\n",
      "gradient only with retain_graph: 2.400\n",
      "#### EXCEPTION:  element 0 of tensors does not require grad and does not have a grad_fn\n"
     ]
    }
   ],
   "source": [
    "input_r, gradient_r = generate_grad(True, False)\n",
    "print(f\"gradient only with retain_graph: {gradient_r:.3f}\")\n",
    "try:\n",
    "    gradient_r.backward()\n",
    "    print(input_r) # Graph did not created therefore cannot do backward() operation\n",
    "except RuntimeError as RE:\n",
    "    print(\"#### EXCEPTION: \", RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.040\n",
      "dloss/dinput:\n",
      " (tensor([[0.6000, 0.6000],\n",
      "        [0.6000, 0.6000]], grad_fn=<MulBackward0>),)\n",
      "gradient only with retain_graph: 2.400\n",
      "#### EXCEPTION:  Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "input_c, gradient_c = generate_grad(False, True)\n",
    "print(f\"gradient only with retain_graph: {gradient_c:.3f}\")\n",
    "try:\n",
    "    gradient_c.backward(retain_graph=True)\n",
    "    print(input_c) # Retain graph option crashed with create graph option, thus buffers are freed\n",
    "except RuntimeError as RE:\n",
    "    print(\"#### EXCEPTION: \", RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.040\n",
      "dloss/dinput:\n",
      " (tensor([[0.6000, 0.6000],\n",
      "        [0.6000, 0.6000]], grad_fn=<MulBackward0>),)\n",
      "gradient only with retain_graph: 2.400\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input_rc, gradient_rc = generate_grad(True, True)\n",
    "print(f\"gradient only with retain_graph: {gradient_rc:.3f}\")\n",
    "try:\n",
    "    gradient_rc.backward()\n",
    "    print(input_rc) # Graph created and retained in backward() operation\n",
    "except RuntimeError as RE:\n",
    "    print(\"#### EXCEPTION: \", RE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- torch official documentation: https://pytorch.org/docs/stable/autograd.html\n",
    "- https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n",
    "- https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method\n",
    "- Gradient, Jacobian, Hessian... : https://darkpgmr.tistory.com/132"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
